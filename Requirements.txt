# Requirements.txt
flask==2.3.3
flask-cors==4.0.0
langchain==0.1.0
langchain-ollama==0.1.0
langchain-chroma==0.1.0
langchain-community==0.0.20
langgraph==0.0.26
langsmith==0.0.83
spacy==3.7.2
PyMuPDF==1.23.14
docx2txt==0.8
fuzzywuzzy==0.18.0
python-Levenshtein==0.21.1
scikit-learn==1.3.2
numpy==1.25.2
pandas==2.1.4
sentence-transformers==2.2.2
chromadb==0.4.22
pydantic==2.5.2

# Setup Instructions

## 1. System Requirements
- Python 3.9+ 
- Ollama installed and running
- 8GB+ RAM (for models)
- 10GB+ disk space

## 2. Install Ollama and Models
```bash
# Download and install Ollama from https://ollama.ai
# Then pull required models:
ollama pull gemma3:1b
ollama pull mxbai-embed-large
ollama pull nomic-embed-text  # Alternative
ollama serve  # Start Ollama server
```

## 3. Install Python Dependencies
```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

## 4. Project Structure
```
resume-jd-matcher/
├── app.py                 # Flask backend
├── index.html            # Frontend HTML
├── Semantics_2.py        # Your semantic matching code
├── Hard_Matching.py      # Your hard matching code
├── requirements.txt      # Dependencies
├── uploads/              # Temporary upload directory
└── chroma_db/           # Vector database storage
```

## 5. Running the Application
```bash
python app.py
```

Open browser to: http://localhost:5000

## 6. API Endpoints

### POST /analyze
Upload JD and multiple resumes for analysis
- Content-Type: multipart/form-data
- Fields: jd_file (single), resume_files (multiple)
- Returns: JSON with analysis results

### GET /health
Check system status and model availability

### GET /models  
List installed Ollama models

## 7. Features
- Hybrid scoring (Semantic + Hard matching)
- Primary embedding: mxbai-embed-large
- Fallback models: nomic-embed-text, sentence-transformers
- Batch processing multiple resumes
- Real-time analytics dashboard
- AI-powered suggestions
- Responsive modern UI
- Vector database storage with Chroma

## 8. Troubleshooting
- Ensure Ollama is running: `ollama serve`
- Check models: `ollama list`
- Verify Python dependencies: `pip list`
- Check logs in terminal for detailed errors
- File size limit: 50MB per file
- Supported formats: PDF, DOCX, DOC

## 9. Performance Notes
- First analysis may take 2-3 minutes (model loading)
- Subsequent analyses are faster (cached models)
- Processing time scales with number of resumes
- mxbai-embed-large provides better accuracy than smaller models